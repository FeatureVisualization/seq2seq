{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def perplexity(label, pred, ignore_label):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        if label[i] == ignore_label:\n",
    "            break\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_train_input='data/1.en'\n",
    "dec_train_input='data/1.ru'\n",
    "# enc_train_input='data/train.en'\n",
    "# dec_train_input='data/train.ru'\n",
    "num_buckets=1\n",
    "num_layers=3\n",
    "num_hidden=10\n",
    "batch_size=1\n",
    "iterations=1\n",
    "expt_name='simple'\n",
    "params_dir='params'\n",
    "shuffle=False\n",
    "reverse=True\n",
    "top_words=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# import numpy as np\n",
    "\n",
    "remove_chars = re.compile(r'([\\(\\)\\'\\\"{}\\[\\]\\*\\-/])')\n",
    "punctuation = re.compile(r'([!,.?:;@#$%&]+)')\n",
    "def filter_text(text):\n",
    "    text = remove_chars.sub(' ', text)\n",
    "    # returns the text without the <EOS> token\n",
    "    text = punctuation.sub(r' \\1 ', text) # replaces the punctuation so that there is a space seperating it from the word\n",
    "    text = text.lower().strip(' \\t\\n') # replaces big caps with small caps\n",
    "    return text\n",
    "\n",
    "white_spaces = re.compile(r'[ \\n\\r\\t]+')\n",
    "def get_vocab(file, vocab_count={}):\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            tokens = white_spaces.split(filter_text(line))\n",
    "            for token in tokens:\n",
    "                if len(token) > 0:\n",
    "                    if token in vocab_count:\n",
    "                        vocab_count[token] += 1\n",
    "                    else:\n",
    "                        vocab_count[token] = 1\n",
    "    return vocab_count\n",
    "\n",
    "def text_2_indices(word2idx, text):\n",
    "    # return the list of indices representing this text including the <EOS> token at the end...\n",
    "    tokens = white_spaces.split(filter_text(text))\n",
    "    indices = []\n",
    "    unk_index = word2idx.get('<UNK>')\n",
    "    indices = [ word2idx.get(token, unk_index) for token in tokens ]\n",
    "    indices.append(word2idx['<EOS>'])\n",
    "    return np.array(indices)\n",
    "\n",
    "def get_unified_vocab(enc_input_file, dec_input_file, percentile=80):\n",
    "    vocab_count = get_vocab(enc_input_file) # this returns a dictionary\n",
    "    vocab_count = get_vocab(dec_input_file, vocab_count) # this returns a dictionary\n",
    "    \n",
    "    word_distribution = np.array( [ v for v in vocab_count.values() ] )\n",
    "    min_count = np.percentile(word_distribution, percentile)\n",
    "    vocab = []\n",
    "    for k,v in vocab_count.items():\n",
    "        if v >= min_count:\n",
    "            vocab.append(k)\n",
    "    vocab.sort()\n",
    "    \n",
    "    vocab.append('<UNK>') # token representing a word unseen in the training set, reserved for rare words\n",
    "    vocab.append('<EOS>') # token representing the End-of-Sentence\n",
    "    vocab.append('<PAD>') # token representing the padding for use in bucketing RNN of different lengths\n",
    "    \n",
    "    word2idx = { w:i for i,w in enumerate(vocab) }\n",
    "    idx2word = [ w for w in vocab ]\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def get_data_label(enc_input_file, dec_input_file, word2idx):\n",
    "    enc_input = []\n",
    "    with open(enc_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            enc_input.append(indices)\n",
    "\n",
    "    dec_input = []\n",
    "    with open(dec_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            dec_input.append(indices)\n",
    "    return np.array( list(zip(enc_input, dec_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_unified_vocab(enc_train_input, dec_train_input, top_words)\n",
    "train_data_label = get_data_label(enc_train_input, dec_train_input, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "LSTMState = namedtuple(\"LSTMState\", [\"c\", \"h\"])\n",
    "LSTMParam = namedtuple(\"LSTMParam\", [\"i2h_weight\", \"i2h_bias\", \"h2h_weight\", \"h2h_bias\"])\n",
    "\n",
    "def lstm_cell(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0.):\n",
    "    \"\"\"LSTM Cell symbol\"\"\"\n",
    "    if dropout > 0.:\n",
    "        indata = mx.sym.Dropout(data=indata, p=dropout)\n",
    "    \n",
    "    i2h = mx.sym.FullyConnected(\n",
    "        data=indata,\n",
    "        weight=param.i2h_weight,\n",
    "        bias=param.i2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_i2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    h2h = mx.sym.FullyConnected(\n",
    "        data=prev_state.h,\n",
    "        weight=param.h2h_weight,\n",
    "        bias=param.h2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_h2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    gates = i2h + h2h\n",
    "    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4, name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n",
    "\n",
    "    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n",
    "    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n",
    "    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n",
    "    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n",
    "\n",
    "    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n",
    "    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n",
    "\n",
    "    return LSTMState(c=next_c, h=next_h)\n",
    "\n",
    "def init_lstm(num_layer):\n",
    "    param_cells = []\n",
    "    last_states = []\n",
    "    for i in range(num_layer):\n",
    "        param_cells.append(\n",
    "            LSTMParam(\n",
    "                i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n",
    "                i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n",
    "                h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n",
    "                h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)\n",
    "            )\n",
    "        )\n",
    "        last_states.append(\n",
    "            LSTMState(\n",
    "                c=mx.sym.Variable(\"l%d_init_c\" % i),\n",
    "                h=mx.sym.Variable(\"l%d_init_h\" % i)\n",
    "            )\n",
    "        )\n",
    "    return param_cells, last_states\n",
    "\n",
    "def lstm_unroll(num_layer, seqlen, num_hidden, num_labels, dropout=0.0):\n",
    "    cls_weight   = mx.sym.Variable(\"cls_weight\")\n",
    "    cls_bias     = mx.sym.Variable(\"cls_bias\")\n",
    "    embed_weight = mx.sym.Variable(\"embed_weight\")\n",
    "\n",
    "    param_cells, last_states = init_lstm(num_layer)\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('label')\n",
    "    \n",
    "    embed = mx.sym.Embedding(\n",
    "        data=data, # the idx to the embedding\n",
    "        input_dim=num_labels, # the number of rows for embed_weight\n",
    "        weight=embed_weight,  # the matrix representing the idx2vec\n",
    "        output_dim=num_hidden, # the number of cols for embed_weight\n",
    "        name='embed'\n",
    "    )\n",
    "    \n",
    "    wordvec = mx.sym.SliceChannel(data=embed, num_outputs=seqlen, squeeze_axis=1)\n",
    "\n",
    "    hidden_all = []\n",
    "    for seqidx in range(seqlen):\n",
    "        hidden = wordvec[seqidx]\n",
    "        # stack LSTM\n",
    "        for i in range(num_layer):\n",
    "            dp = 0.0 if i == 0 else dropout\n",
    "            next_state = lstm_cell(\n",
    "                num_hidden,\n",
    "                indata=hidden,\n",
    "                prev_state=last_states[i],\n",
    "                param=param_cells[i],\n",
    "                seqidx=seqidx,\n",
    "                layeridx=i,\n",
    "                dropout=dp\n",
    "            )\n",
    "            hidden = next_state.h\n",
    "            last_states[i] = next_state\n",
    "        # decoder\n",
    "        if dropout > 0.0:\n",
    "            hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "        \n",
    "        hidden_all.append(hidden)\n",
    "        \n",
    "    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n",
    "    pred = mx.sym.FullyConnected(\n",
    "        data=hidden_concat,\n",
    "        num_hidden=num_labels, # num_labels is the index of <PAD> that means this layer will predict 0, 1, ..., num_labels-1\n",
    "        weight=cls_weight,\n",
    "        bias=cls_bias,\n",
    "        name='pred'\n",
    "    )\n",
    "\n",
    "    label = mx.sym.transpose(data=label) # e.g. if shape is (1,M) it becomes (M,1)\n",
    "    label = mx.sym.Reshape(data=label, shape=(-1,)) # if shape is (M,1) it becomes (M,)\n",
    "    output = mx.sym.SoftmaxOutput(\n",
    "        data=pred,\n",
    "        label=label,\n",
    "        name='t%d_softmax' % seqidx,\n",
    "        use_ignore=True,\n",
    "        ignore_label=num_labels # ignore the index of <PAD>\n",
    "    ) # output becomes (num_labels, M)\n",
    "    return output\n",
    "\n",
    "def get_lstm_sym_generator(num_layers, num_hidden, num_labels, dropout=0.0):\n",
    "    def generate_lstm_sym(seqlen):\n",
    "        return lstm_unroll(num_layers, seqlen, num_hidden, num_labels, dropout)\n",
    "    return generate_lstm_sym\n",
    "\n",
    "def get_lstm_init_states(num_layers, num_dim, batch_size=1):\n",
    "    init_h = [('l%d_init_h' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_c = [('l%d_init_c' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_states = init_h + init_c\n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class EncoderDecoderBatch(object):\n",
    "    def __init__(self, all_data, all_label, init_states, bucket_key):\n",
    "        self.pad = 0 # at this point i do not know what is this for...\n",
    "        \n",
    "        #all_data.shape is (x,y,z)\n",
    "        self.batch_size = all_data.shape[0]\n",
    "        \n",
    "        # provide data, essential assignment\n",
    "        self.data = [ mx.nd.array(all_data) ]\n",
    "        \n",
    "        # essential assignment\n",
    "        self.provide_data = [('data', (self.batch_size, bucket_key))]\n",
    "        for x in init_states:\n",
    "            self.data.append(mx.nd.zeros(x[1])) # x[1] is the shape of the initial data\n",
    "            self.provide_data.append(x)\n",
    "\n",
    "        # provide label, essential assignment\n",
    "        self.label = [ mx.nd.array(all_label) ]\n",
    "        self.provide_label = [ ('label', (self.batch_size, bucket_key)) ]\n",
    "        \n",
    "        self.init_states = init_states\n",
    "\n",
    "        # bucket_key is essential for this databatch\n",
    "        self.bucket_key = bucket_key\n",
    "\n",
    "def synchronize_batch_size(train_iter, test_iter):\n",
    "    batch_size = min(train_iter.batch_size, test_iter.batch_size)\n",
    "    train_iter.batch_size = batch_size\n",
    "    test_iter.batch_size = batch_size\n",
    "    train_iter.generate_init_states()\n",
    "    test_iter.generate_init_states()\n",
    "\n",
    "# now define the bucketing, padding and batching SequenceIterator...\n",
    "class EncoderDecoderIter(mx.io.DataIter):\n",
    "    def __init__(self, data_label, word2idx, idx2word, num_hidden, num_layers,\n",
    "                 init_states_function, batch_size=1, num_buckets=10, shuffle=False, rev=False):\n",
    "\n",
    "        super(EncoderDecoderIter, self).__init__() # calling DataIter.__init__()\n",
    "\n",
    "        # data is a numpy array of 3 dimensions, #, timesteps, vector_dim\n",
    "        self.data_label = data_label\n",
    "\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "        # now we need to find the buckets based on the input data...\n",
    "        self.buckets, self.buckets_count, self.assignments = self.generate_buckets()\n",
    "        # buckets are a tuple of the encoder/decoder length\n",
    "\n",
    "        self.batch_size = min(np.min(self.buckets_count), batch_size)\n",
    "        self.init_states_function = init_states_function\n",
    "        self.pad_label = word2idx['<PAD>']\n",
    "        self.shuffle = shuffle\n",
    "        self.rev = rev # reverse the encoder input\n",
    "        self.reset()\n",
    "        self.generate_init_states()\n",
    "\n",
    "    def generate_init_states(self):\n",
    "        self.init_states = self.init_states_function(self.num_layers, self.num_hidden, self.batch_size)\n",
    "\n",
    "    def generate_buckets(self):\n",
    "        enc_dec_data = [ len(data)+len(label)-1 for data, label in self.data_label ]\n",
    "        enc_dec_data = np.reshape(np.array(enc_dec_data), (-1, 1))\n",
    "\n",
    "        kmeans = KMeans(n_clusters=self.num_buckets, random_state=1) # use clustering to decide the buckets\n",
    "        assignments = kmeans.fit_predict(enc_dec_data) # get the assignments\n",
    "\n",
    "        # get the max of every cluster\n",
    "        buckets = np.array([np.amax(enc_dec_data[assignments==i]) for i in range(self.num_buckets)])\n",
    "\n",
    "        # get # of sequences in each bucket... then assign the batch size as the minimum(minimum(bucketsize), batchsize)\n",
    "        buckets_count = np.array([enc_dec_data[assignments==i].shape[0] for i in range(self.num_buckets)])\n",
    "\n",
    "        return buckets, buckets_count, assignments\n",
    "\n",
    "    @property\n",
    "    def default_bucket_key(self):\n",
    "        return np.amax(self.buckets)\n",
    "\n",
    "    @property\n",
    "    def provide_data(self): # this is necessary when specifying custom DataIter\n",
    "        # length of data variable is length of encoder + length of decoder\n",
    "        bucket_key = self.default_bucket_key\n",
    "        return [('data', (self.batch_size, bucket_key))] + self.init_states\n",
    "    #\n",
    "    @property\n",
    "    def provide_label(self): # this is necessary when specifying custom DataIter\n",
    "        # length of label variable is only the length of decoder\n",
    "        bucket_key = self.default_bucket_key\n",
    "        return [('label', (self.batch_size, bucket_key))]\n",
    "\n",
    "    # for custom DataIter, we must implement this class as an iterable and return a DataBatch\n",
    "    def __iter__(self): # this is necessary to convert this class into an iterable\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter_next():\n",
    "            # suppose to get self.cursor:self.cursor + self.batch_size\n",
    "            batch = self.data_label[self.assignments == self.cur_permute_bucket]\\\n",
    "                [self.in_bucket_permutation[self.cursor:self.cursor+self.batch_size]]\n",
    "\n",
    "            # get size of this bucket\n",
    "            seqlen = self.buckets[self.cur_permute_bucket] # this seqlen already deducted the <EOS>\n",
    "\n",
    "            all_data = np.full((self.batch_size, seqlen), self.pad_label, dtype=float)\n",
    "            all_label = np.full((self.batch_size, seqlen), self.pad_label, dtype=float)\n",
    "\n",
    "            for i, (data, label) in enumerate(batch):\n",
    "                if self.rev:\n",
    "                    # reverse the input except for the <EOS> at end of input\n",
    "                    # according to Ilya Sutskever et al. Sequence to Sequence Learning with Neural Networks\n",
    "                    # there is a reason for this... which you should ask freddy\n",
    "                    data[:-1] = np.flipud(data[:-1])\n",
    "\n",
    "                all_data[i, :data.shape[0]] = data\n",
    "                all_data[i, data.shape[0]:data.shape[0]+label.shape[0]-1] = label[:-1]\n",
    "                all_label[i, data.shape[0]-1:data.shape[0]-1+label.shape[0]] = label\n",
    "\n",
    "            return EncoderDecoderBatch(all_data, all_label, self.init_states, seqlen)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def iter_next(self):\n",
    "        self.cursor += self.batch_size\n",
    "        if self.cursor < self.buckets_count[self.cur_permute_bucket]:\n",
    "            if self.cursor + self.batch_size > self.buckets_count[self.cur_permute_bucket]:\n",
    "                # it is going to overflow the bucket\n",
    "                self.cursor -= self.cursor + self.batch_size - self.buckets_count[self.cur_permute_bucket]\n",
    "            return True\n",
    "        else:\n",
    "            self.cur_bucket += 1\n",
    "            if self.cur_bucket < self.num_buckets:\n",
    "                self.cursor = 0\n",
    "                self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "                if self.shuffle:\n",
    "                    self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "                else:\n",
    "                    self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def reset(self): # for iterable\n",
    "        self.cursor = -self.batch_size\n",
    "        self.cur_bucket = 0\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.bucket_permutation = np.random.permutation(self.num_buckets)\n",
    "        else:\n",
    "            self.bucket_permutation = np.array(range(self.num_buckets))\n",
    "\n",
    "        self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "        if self.shuffle:\n",
    "            self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "        else:\n",
    "            self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_iter = EncoderDecoderIter(train_data_label, word2idx, idx2word,\n",
    "            num_hidden, num_layers, get_lstm_init_states, batch_size=batch_size,\n",
    "            num_buckets=num_buckets, shuffle=shuffle, rev=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_iter(iter):\n",
    "    iter.reset()\n",
    "    print('provide_data: ', iter.provide_data)\n",
    "    print('provide_label: ', iter.provide_label)\n",
    "    print('buckets: ', iter.buckets)\n",
    "    print('buckets count: ', iter.buckets_count)\n",
    "    print('assignments: ', iter.assignments)\n",
    "    print('batch_size: ', iter.batch_size)\n",
    "    for i, data_batch in enumerate(iter):\n",
    "        print(i, data_batch.provide_data)\n",
    "        print(i, data_batch.provide_label)\n",
    "        print(i, data_batch.bucket_key)\n",
    "#         print(i, data_batch.data)\n",
    "        for j, d in enumerate(data_batch.data):\n",
    "#             print(i, j, data_batch.data[j].shape)\n",
    "            if j==0:\n",
    "                print(i, 'data:', data_batch.data[j].asnumpy())\n",
    "#         print(i, data_batch.label)\n",
    "#         print(i, data_batch.label[0].shape)\n",
    "        print(i, 'label:', data_batch.label[0].asnumpy())\n",
    "#         print('\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "context = mx.cpu()\n",
    "\n",
    "model_args = {}\n",
    "if os.path.isfile('%s/%s-symbol.json' % (params_dir, expt_name)):\n",
    "    filelist = os.listdir(params_dir) # get list of params file\n",
    "    paramfilelist = []\n",
    "    for f in filelist:\n",
    "        if f.startswith('%s-' % expt_name) and f.endswith('.params'):\n",
    "            paramfilelist.append( int(re.split(r'[-.]', f)[1]) )\n",
    "    last_iteration = max(paramfilelist)\n",
    "    print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "    tmp = mx.model.FeedForward.load('%s/%s' % (params_dir, expt_name), last_iteration)\n",
    "    model_args.update({\n",
    "        'arg_params' : tmp.arg_params,\n",
    "        'aux_params' : tmp.aux_params,\n",
    "        'begin_epoch' : tmp.begin_epoch\n",
    "    })\n",
    "\n",
    "num_labels = len(word2idx)\n",
    "iterations = 1000\n",
    "model = mx.model.FeedForward(\n",
    "    ctx           = context, # uses all the available CPU in the machine\n",
    "    symbol        = get_lstm_sym_generator(num_layers, num_hidden, num_labels),\n",
    "    num_epoch     = iterations,\n",
    "    learning_rate = 0.1,\n",
    "    momentum      = 0.0,\n",
    "    wd            = 0.00001,\n",
    "    initializer   = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "    **model_args\n",
    ")\n",
    "\n",
    "if not os.path.exists(params_dir):\n",
    "    os.makedirs(params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric = mx.metric.np(perplexity, use_ignore=True, ignore_label=num_labels),\n",
    "    batch_end_callback = [ mx.callback.Speedometer(batch_size, frequent=10) ],\n",
    "    epoch_end_callback = [ mx.callback.do_checkpoint( '%s/%s' % (params_dir, expt_name) ) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_dir = 'params'\n",
    "expt_name  = 'simple'\n",
    "num_layers = 3\n",
    "num_hidden = 10\n",
    "num_labels = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_iteration = 1000\n",
    "print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "_, arg_params, __ = mx.model.load_checkpoint('%s/%s' % (params_dir, expt_name), last_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lstm_inference_symbol(num_layer, num_hidden, num_labels, dropout=0.0):\n",
    "    param_cells, last_states = init_lstm(num_layer)\n",
    "    \n",
    "    data = mx.sym.Variable('data')\n",
    "    embed_weight=mx.sym.Variable(\"embed_weight\")\n",
    "    \n",
    "    hidden = mx.sym.Embedding(data=data, input_dim=num_labels, weight=embed_weight, output_dim=num_hidden, name='embed')\n",
    "    \n",
    "    # stack layers of LSTM for 1 sequence\n",
    "    for i in range(num_layer):\n",
    "        dp = 0.0 if i == 0 else dropout\n",
    "        next_state = lstm_cell(\n",
    "            num_hidden,\n",
    "            indata=hidden,\n",
    "            prev_state=last_states[i],\n",
    "            param=param_cells[i],\n",
    "            seqidx=0,\n",
    "            layeridx=i,\n",
    "            dropout=dp\n",
    "        )\n",
    "        hidden = next_state.h\n",
    "        last_states[i] = next_state\n",
    "    \n",
    "    if dropout > 0.0:\n",
    "        hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "    \n",
    "    output = []\n",
    "    for state in last_states:\n",
    "        # very important to be in this order!!!\n",
    "        output.append(state.h)\n",
    "        output.append(state.c)\n",
    "    \n",
    "    return mx.sym.Group(output)\n",
    "\n",
    "class LSTMInferenceModel(object):\n",
    "    def __init__(self, num_layer, num_hidden, num_labels, arg_params, ctx=mx.cpu(), dropout=0.0):\n",
    "        \n",
    "        self.sym = lstm_inference_symbol(num_layer, num_hidden, num_labels, dropout)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        batch_size = 1\n",
    "        init_states = get_lstm_init_states(num_layer, num_hidden, batch_size)\n",
    "        data_shape = [(\"data\", (batch_size, ))]\n",
    "\n",
    "        input_shapes = dict(init_states + data_shape)\n",
    "        self.executor = self.sym.simple_bind(ctx=ctx, **input_shapes)\n",
    "\n",
    "        # copy the transition parameters over to executor\n",
    "        for key in self.executor.arg_dict.keys():\n",
    "            if key in arg_params:\n",
    "                arg_params[key].copyto(self.executor.arg_dict[key])\n",
    "\n",
    "        state_name = []\n",
    "        for i in range(num_layer):\n",
    "            # very important to be in this order!!!\n",
    "            state_name.append(\"l%d_init_h\" % i)\n",
    "            state_name.append(\"l%d_init_c\" % i)\n",
    "\n",
    "        self.states_dict = dict(zip(state_name, self.executor.outputs)) # this transfer the output of previous state to current\n",
    "\n",
    "        self.cls_weight = arg_params['cls_weight']\n",
    "        self.cls_bias   = arg_params['cls_bias']\n",
    "        self.ctx = ctx\n",
    "\n",
    "    def predict(self, x):\n",
    "        # another symbolic graph here... \n",
    "        data       = mx.sym.Variable('data')\n",
    "        cls_weight = mx.sym.Variable(\"cls_weight\")\n",
    "        cls_bias   = mx.sym.Variable(\"cls_bias\")\n",
    "    \n",
    "        pred = mx.sym.FullyConnected(\n",
    "            data       = data,\n",
    "            num_hidden = self.num_labels,\n",
    "            weight     = cls_weight,\n",
    "            bias       = cls_bias,\n",
    "            name       = 'pred'\n",
    "        )\n",
    "        \n",
    "        output = mx.sym.SoftmaxOutput(\n",
    "            data = pred,\n",
    "            name = 'softmax'\n",
    "        )\n",
    "        \n",
    "        executor = output.bind(ctx=self.ctx, args={\n",
    "            'data': x,\n",
    "            'cls_weight': self.cls_weight,\n",
    "            'cls_bias'  : self.cls_bias,\n",
    "            'softmax_label': mx.nd.array([0]) # this is a dummy label, just meant to fulfill the requirements...\n",
    "        })\n",
    "        \n",
    "        executor.forward()\n",
    "        prob = np.squeeze(executor.outputs[0].asnumpy())\n",
    "        return prob\n",
    "        \n",
    "    def forward(self, input_data, new_seq=False):\n",
    "        # input data is of shape (seqlen, dim)\n",
    "        # input data has to be of type numpy.array\n",
    "        if new_seq == True:\n",
    "            # this is meant to reset the initial states to 0.0\n",
    "            for key in self.states_dict.keys():\n",
    "                self.executor.arg_dict[key][:] = 0.0\n",
    "        \n",
    "        for x in input_data:\n",
    "            y = mx.nd.array([x]) # put it in a [] so that the shape becomes (1, xxx)\n",
    "            y.copyto(self.executor.arg_dict[\"data\"])\n",
    "            self.executor.forward() # move forward one step...\n",
    "            for key in self.states_dict.keys():\n",
    "                # copy the hidden and c to the init_states for the next sequence\n",
    "                self.states_dict[key].copyto(self.executor.arg_dict[key])\n",
    "        \n",
    "        return self.predict(self.states_dict['l2_init_h']) # change this to use last layer next time...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = LSTMInferenceModel(num_layers, num_hidden, num_labels, arg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the word...\n",
    "def get_word(prob, idx2word, sample=True):\n",
    "    if sample:\n",
    "        cdf = np.cumsum(prob) / np.sum(prob)\n",
    "        idx = np.argmax(np.random.rand(1) < cdf)\n",
    "    else:\n",
    "        idx = np.argmax(prob)\n",
    "    return idx, idx2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate(text, model2, idx2word, reverse=True):\n",
    "    data = text_2_indices(word2idx, text)\n",
    "    if reverse:\n",
    "        data[:-1] = np.flipud(data[:-1])\n",
    "    eos_idx = word2idx['<EOS>']\n",
    "    \n",
    "    words = ''\n",
    "    prob = model2.forward(data, new_seq=True)\n",
    "    idx, word = get_word(prob, idx2word, sample=True)\n",
    "    while idx != eos_idx:\n",
    "        words += word + ' '\n",
    "        prob = model2.forward(np.array([idx]))\n",
    "        idx, word = get_word(prob, idx2word, sample=True)\n",
    "    \n",
    "    return words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate('i just ate my dinner', model2, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "я только что съел мой ужин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate('good morning', model2, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arg_params['embed_weight'].asnumpy()[word2idx['<PAD>']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
