{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_file = 'data/news-commentary-v9.ru-en.en'\n",
    "russian_file = 'data/news-commentary-v9.ru-en.ru'\n",
    "\n",
    "train_en_file = 'data/train.en'\n",
    "train_ru_file = 'data/train.ru'\n",
    "\n",
    "test_en_file = 'data/test.en'\n",
    "test_ru_file = 'data/test.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_proportion = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'([!,.?:;@#$%&\\'\\\"]+)')\n",
    "def filter_text(text):\n",
    "    # this matches the function in scala for processing the text\n",
    "    text = punctuation.sub(r' \\1 ', text) # replaces the 4 punctuation so that there is a space seperating it from the word\n",
    "    text = text.lower().strip(' \\t\\n') # replaces big caps with small caps\n",
    "    text = text + ' <EOS>'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_en_fid = open(train_en_file, 'w', encoding='utf-8')\n",
    "train_ru_fid = open(train_ru_file, 'w', encoding='utf-8')\n",
    "\n",
    "test_en_fid = open(test_en_file, 'w', encoding='utf-8')\n",
    "test_ru_fid = open(test_ru_file, 'w', encoding='utf-8')\n",
    "\n",
    "fid_en = open(english_file, 'r', encoding='utf-8', errors='ignore')\n",
    "fid_ru = open(russian_file, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "for line_en in fid_en:\n",
    "    line_en = line_en.strip(' \\n\\t')\n",
    "    line_ru = fid_ru.readline()\n",
    "    if len(line_en) == 0:\n",
    "        continue\n",
    "    \n",
    "    line_en = line_en + '\\n'\n",
    "    line_ru = line_ru.strip(' \\n\\t') + '\\n'\n",
    "    \n",
    "    if np.random.rand() < train_proportion:\n",
    "        train_en_fid.write(line_en)\n",
    "        train_ru_fid.write(line_ru)\n",
    "    else:\n",
    "        test_en_fid.write(line_en)\n",
    "        test_ru_fid.write(line_ru)\n",
    "\n",
    "train_en_fid.close()\n",
    "test_en_fid.close()\n",
    "train_ru_fid.close()\n",
    "test_ru_fid.close()\n",
    "fid_en.close()\n",
    "fid_ru.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "white_spaces = re.compile(r'[ \\n\\r\\t]+')\n",
    "\n",
    "def get_vocab(file, vocab_count):\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            line2 = filter_text(line)\n",
    "            tokens = white_spaces.split(line2)\n",
    "            for token in tokens:\n",
    "                if len(token) > 0:\n",
    "                    if token in vocab_count:\n",
    "                        vocab_count[token] += 1\n",
    "                    else:\n",
    "                        vocab_count[token] = 1              \n",
    "    return vocab_count\n",
    "\n",
    "def text_2_indices(word2idx, text):\n",
    "    tokens = white_spaces.split(filter_text(text)) #add the <EOS> and split it...\n",
    "    indices = []\n",
    "    unk_index = word2idx.get('UNK')\n",
    "    indices = [ word2idx.get(token, unk_index) for token in tokens ]\n",
    "    return np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unified_vocab(enc_input_file, dec_input_file):\n",
    "    vocab_count = {}\n",
    "    vocab_count = get_vocab(enc_input_file, vocab_count) # this returns a dictionary\n",
    "    vocab_count = get_vocab(dec_input_file, vocab_count) # this returns a dictionary\n",
    "    \n",
    "    word_distribution = np.array( [ v for v in vocab_count.values() ] )\n",
    "    min_count = np.percentile(word_distribution, 80)\n",
    "    vocab = []\n",
    "    for k,v in vocab_count.items():\n",
    "        if v >= min_count:\n",
    "            vocab.append(k)\n",
    "    vocab.append('UNK')\n",
    "    vocab.sort()\n",
    "    word2idx = { w:i for i,w in enumerate(vocab) }\n",
    "    idx2word = [ w for w in vocab ] #{ v:k for k,v in word2idx.items() }\n",
    "\n",
    "    pad_index = len(word2idx)\n",
    "    word2idx['<PAD>'] = pad_index\n",
    "    idx2word.append('<PAD>')\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def get_data_label(enc_input_file, dec_input_file, word2idx):\n",
    "    enc_input = []\n",
    "    with open(enc_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            enc_input.append(indices)\n",
    "    dec_input = []\n",
    "    with open(dec_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            dec_input.append(indices)\n",
    "    return np.array( list(zip(enc_input, dec_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_unified_vocab(train_en_file, train_ru_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5529"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['electing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\uf0b7'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'electing': 5529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_init_states(num_layers, num_dim, batch_size=1):\n",
    "    init_h = [('l%d_init_h' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_c = [('l%d_init_c' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_states = init_h + init_c\n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "EncDecBucketKey = namedtuple('EncDecBucketKey', ['enc_len', 'dec_len'])\n",
    "\n",
    "class EncoderDecoderBatch(object):\n",
    "    def __init__(self, all_data, all_label, init_states, bucket_key):\n",
    "        # provide data, essential assignment\n",
    "        self.data = [ mx.nd.array(all_data) ] + [ mx.nd.zeros(x[1]) for x in init_states ]\n",
    "\n",
    "        # provide label, essential assignment\n",
    "        self.label = [ mx.nd.array(all_label) ]\n",
    "\n",
    "        self.init_states = init_states\n",
    "\n",
    "        # bucket_key is essential for this databatch\n",
    "        self.bucket_key = bucket_key\n",
    "\n",
    "        #all_data.shape is (x,y,z)\n",
    "        self.batch_size = all_data.shape[0]\n",
    "\n",
    "    # this two properties are essential too!\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [('data', (self.batch_size, self.bucket_key.enc_len + self.bucket_key.dec_len))] + self.init_states\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return [('label', (self.batch_size, self.bucket_key.dec_len))]\n",
    "\n",
    "\n",
    "def synchronize_batch_size(train_iter, test_iter):\n",
    "    batch_size = min(train_iter.batch_size, test_iter.batch_size)\n",
    "    train_iter.batch_size = batch_size\n",
    "    test_iter.batch_size = batch_size\n",
    "    train_iter.generate_init_states()\n",
    "    test_iter.generate_init_states()\n",
    "\n",
    "\n",
    "# now define the bucketing, padding and batching SequenceIterator...\n",
    "class EncoderDecoderIter(mx.io.DataIter):\n",
    "    def __init__(self, data_label, word2idx, idx2word, num_hidden, num_layers, \n",
    "                 init_states_function, batch_size=1, num_buckets=10, shuffle=False, rev=False):\n",
    "\n",
    "        super(EncoderDecoderIter, self).__init__() # calling DataIter.__init__()\n",
    "\n",
    "        # data is a numpy array of 3 dimensions, #, timesteps, vector_dim\n",
    "        self.data_label = data_label\n",
    "        \n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "        # arrange the data so that\n",
    "\n",
    "        # now we need to find the buckets based on the input data...\n",
    "        self.buckets, self.buckets_count, self.assignments = self.generate_buckets()\n",
    "        # buckets are a tuple of the encoder/decoder length\n",
    "\n",
    "        self.batch_size = min(np.min(self.buckets_count), batch_size)\n",
    "        self.init_states_function = init_states_function\n",
    "        self.pad_label = word2idx['<PAD>']\n",
    "        self.shuffle = shuffle\n",
    "        self.rev = rev # reverse the encoder input\n",
    "        self.reset()\n",
    "        self.generate_init_states()\n",
    "\n",
    "    def generate_init_states(self):\n",
    "        self.init_states = self.init_states_function(self.num_layers, self.num_hidden, self.batch_size)\n",
    "\n",
    "    def generate_buckets(self):\n",
    "        enc_dec_data = []\n",
    "        for data, label in data_label:\n",
    "            enc_len = len(data) - 1 # minue one because of the <EOS>\n",
    "            dec_len = len(label)\n",
    "            enc_dec_data.append((enc_len, dec_len))\n",
    "\n",
    "        enc_dec_data = np.array(enc_dec_data)\n",
    "\n",
    "        kmeans = KMeans(n_clusters = self.num_buckets, random_state = 1) # use clustering to decide the buckets\n",
    "        assignments = kmeans.fit_predict(enc_dec_data) # get the assignments\n",
    "\n",
    "        # get the max of every cluster\n",
    "        buckets = np.array([np.max( enc_dec_data[assignments==i], axis=0 ) for i in range(self.num_buckets)])\n",
    "\n",
    "        # get # of sequences in each bucket... then assign the batch size as the minimum(minimum(bucketsize), batchsize)\n",
    "        buckets_count = np.array( [ enc_dec_data[assignments==i].shape[0] for i in range(self.num_buckets) ] )\n",
    "\n",
    "        return buckets, buckets_count, assignments\n",
    "\n",
    "    @property\n",
    "    def default_bucket_key(self):\n",
    "        enc_len, dec_len = np.max(self.buckets, axis=0)\n",
    "        return EncDecBucketKey(enc_len = enc_len, dec_len = dec_len)\n",
    "\n",
    "    @property\n",
    "    def provide_data(self): # this is necessary when specifying custom DataIter\n",
    "        # length of data variable is length of encoder + length of decoder\n",
    "        enc_dec_bucket_key = self.default_bucket_key\n",
    "\n",
    "        return [('data', (self.batch_size, enc_dec_bucket_key.enc_len + enc_dec_bucket_key.dec_len))] + self.init_states\n",
    "    #\n",
    "    @property\n",
    "    def provide_label(self): # this is necessary when specifying custom DataIter\n",
    "        # length of label variable is only the length of decoder\n",
    "        enc_dec_bucket_key = self.default_bucket_key\n",
    "        return [('label', (self.batch_size, enc_dec_bucket_key.dec_len))]\n",
    "    \n",
    "    # for custom DataIter, we must implement this class as an iterable and return a DataBatch\n",
    "    def __iter__(self): # this is necessary to convert this class into an iterable\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.iter_next():\n",
    "            # suppose to get self.cursor:self.cursor + self.batch_size\n",
    "            batch = self.data_label[self.assignments == self.cur_permute_bucket]\\\n",
    "                [ self.in_bucket_permutation[self.cursor:self.cursor+self.batch_size] ]\n",
    "\n",
    "            # get size of this bucket\n",
    "            enc_len, dec_len = self.buckets[self.cur_permute_bucket] # this enc_len already deducted the <EOS>\n",
    "            # total length of rnn sequence is enc_len+dec_len\n",
    "\n",
    "            all_data = np.full((self.batch_size, enc_len+dec_len), self.pad_label, dtype=float)\n",
    "            all_label = np.full((self.batch_size, dec_len), self.pad_label, dtype=float)\n",
    "\n",
    "            for i, (data, label) in enumerate(batch):\n",
    "                if self.rev:\n",
    "                    # reverse the input except for the <EOS> at end of input\n",
    "                    # according to Ilya Sutskever et al. Sequence to Sequence Learning with Neural Networks\n",
    "                    data[:-1] = np.flipud(data[:-1])\n",
    "\n",
    "                enc_input = np.concatenate((data, label[:-1])) # data <EOS> label\n",
    "                z = enc_len - data.shape[0] + 1\n",
    "                all_data[i, z:enc_len + label.shape[0]] = enc_input\n",
    "                all_label[i, :label.shape[0]] = label\n",
    "\n",
    "            return EncoderDecoderBatch(all_data, all_label, self.init_states, EncDecBucketKey(enc_len=enc_len, dec_len=dec_len))\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def iter_next(self):\n",
    "        self.cursor += self.batch_size\n",
    "        if self.cursor < self.buckets_count[self.cur_permute_bucket]:\n",
    "            if self.cursor + self.batch_size > self.buckets_count[self.cur_permute_bucket]:\n",
    "                # it is going to overflow the bucket\n",
    "                self.cursor -= self.cursor + self.batch_size - self.buckets_count[self.cur_permute_bucket]\n",
    "            return True\n",
    "        else:\n",
    "            self.cur_bucket += 1\n",
    "            if self.cur_bucket < self.num_buckets:\n",
    "                self.cursor = 0\n",
    "                self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "                if self.shuffle:\n",
    "                    self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "                else:\n",
    "                    self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def reset(self): # for iterable\n",
    "        self.cursor = -self.batch_size\n",
    "        self.cur_bucket = 0\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.bucket_permutation = np.random.permutation(self.num_buckets)\n",
    "        else:\n",
    "            self.bucket_permutation = np.array(range(self.num_buckets))\n",
    "\n",
    "        self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "        if self.shuffle:\n",
    "            self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "        else:\n",
    "            self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_buckets = 10\n",
    "num_layers  = 1\n",
    "batch_size  = 10\n",
    "iterations  = 1\n",
    "expt_name   = 'test'\n",
    "params_dir  = 'params'\n",
    "seed        = 1\n",
    "shuffle     = True\n",
    "rev         = True\n",
    "dropout     = 0.0\n",
    "context     = mx.cpu()\n",
    "num_hidden  = 10\n",
    "\n",
    "num_labels = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_iter(iter):\n",
    "    iter.reset()\n",
    "    print('provide_data: ', iter.provide_data)\n",
    "    print('provide_label: ', iter.provide_label)\n",
    "    print('buckets: ', iter.buckets)\n",
    "    print('buckets count: ', iter.buckets_count)\n",
    "    print('assignments: ', iter.assignments)\n",
    "    print('batch_size: ', iter.batch_size)\n",
    "    for i, data_batch in enumerate(iter):\n",
    "        print(i, data_batch.provide_data)\n",
    "        print(i, data_batch.provide_label)\n",
    "        print(i, data_batch.bucket_key)\n",
    "        print(i, data_batch.data)\n",
    "        for j, d in enumerate(data_batch.data):\n",
    "            print(i, j, data_batch.data[j].shape)\n",
    "        print(i, data_batch.label)\n",
    "        print(i, data_batch.label[0].shape)\n",
    "#         print(i, data_batch.label[0].asnumpy())\n",
    "#         print('\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_iter = EncoderDecoderIter(\n",
    "    data_label, word2idx, idx2word, num_hidden, num_layers, \n",
    "    get_lstm_init_states, batch_size=1, num_buckets=num_buckets, shuffle=shuffle, rev=rev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTMState = namedtuple(\"LSTMState\", [\"c\", \"h\"])\n",
    "LSTMParam = namedtuple(\"LSTMParam\", [\"i2h_weight\", \"i2h_bias\", \"h2h_weight\", \"h2h_bias\"])\n",
    "\n",
    "def lstm_cell(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0.):\n",
    "    \"\"\"LSTM Cell symbol\"\"\"\n",
    "    if dropout > 0.:\n",
    "        indata = mx.sym.Dropout(data=indata, p=dropout)\n",
    "    i2h = mx.sym.FullyConnected(\n",
    "        data=indata,\n",
    "        weight=param.i2h_weight,\n",
    "        bias=param.i2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_i2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    h2h = mx.sym.FullyConnected(\n",
    "        data=prev_state.h,\n",
    "        weight=param.h2h_weight,\n",
    "        bias=param.h2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_h2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    gates = i2h + h2h\n",
    "    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4, name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n",
    "\n",
    "    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n",
    "    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n",
    "    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n",
    "    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n",
    "\n",
    "    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n",
    "    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n",
    "\n",
    "    return LSTMState(c=next_c, h=next_h)\n",
    "\n",
    "def init_lstm(num_lstm_layer):\n",
    "    param_cells = []\n",
    "    last_states = []\n",
    "    for i in range(num_lstm_layer):\n",
    "        param_cells.append(\n",
    "            LSTMParam(\n",
    "                i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n",
    "                i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n",
    "                h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n",
    "                h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)\n",
    "            )\n",
    "        )\n",
    "        last_states.append(\n",
    "            LSTMState(\n",
    "                c=mx.sym.Variable(\"l%d_init_c\" % i),\n",
    "                h=mx.sym.Variable(\"l%d_init_h\" % i)\n",
    "            )\n",
    "        )\n",
    "    return param_cells, last_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_unroll(num_lstm_layer, enc_len, dec_len, num_hidden, num_labels, dropout=0.0):\n",
    "    \n",
    "    cls_weight = mx.sym.Variable(\"cls_weight\")\n",
    "    cls_bias = mx.sym.Variable(\"cls_bias\")\n",
    "    embed_weight=mx.sym.Variable(\"embed_weight\")\n",
    "    \n",
    "    param_cells, last_states = init_lstm(num_lstm_layer)\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('label')\n",
    "    # (batch, time, vec) so axis 1 is the time step\n",
    "    \n",
    "    embed = mx.sym.Embedding(\n",
    "        data=data, input_dim=num_labels,\n",
    "        weight=embed_weight, output_dim=num_hidden, name='embed'\n",
    "    )\n",
    "    wordvec = mx.sym.SliceChannel(data=embed, num_outputs=enc_len + dec_len, squeeze_axis=1)\n",
    "    \n",
    "    hidden_all = []\n",
    "    for seqidx in range(enc_len + dec_len):\n",
    "        hidden = wordvec[seqidx]\n",
    "        # stack LSTM\n",
    "        for i in range(num_lstm_layer):\n",
    "            dp = 0.0 if i == 0 else dropout\n",
    "            next_state = lstm_cell(\n",
    "                num_hidden,\n",
    "                indata=hidden,\n",
    "                prev_state=last_states[i],\n",
    "                param=param_cells[i],\n",
    "                seqidx=seqidx,\n",
    "                layeridx=i,\n",
    "                dropout=dp\n",
    "            )\n",
    "            hidden = next_state.h\n",
    "            last_states[i] = next_state\n",
    "        # decoder\n",
    "        if dropout > 0.0:\n",
    "            hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "        if seqidx >= enc_len:\n",
    "            hidden_all.append(hidden)\n",
    "    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n",
    "    pred = mx.sym.FullyConnected(\n",
    "        data=hidden_concat,\n",
    "        num_hidden=num_labels, # num_labels is the index of <PAD> that means this layer will predict 0, 1, ..., num_labels-1\n",
    "        weight=cls_weight,\n",
    "        bias=cls_bias,\n",
    "        name='pred'\n",
    "    )\n",
    "\n",
    "    label = mx.sym.transpose(data=label) # e.g. if shape is (1,M) it becomes (M,1)\n",
    "    label = mx.sym.Reshape(data=label, shape=(-1,)) # if shape is (M,1) it becomes (M,)\n",
    "    output = mx.sym.SoftmaxOutput(\n",
    "        data=pred,\n",
    "        label=label,\n",
    "        name='t%d_softmax' % seqidx,\n",
    "        use_ignore=True,\n",
    "        ignore_label=num_labels # ignore the index of <PAD>\n",
    "    ) # output becomes (num_labels, M)\n",
    "    return output\n",
    "\n",
    "def get_lstm_sym_generator(num_layers, dim, num_labels, dropout=0.0):\n",
    "    def generate_lstm_sym(bucketkey):\n",
    "        return lstm_unroll(num_layers, bucketkey.enc_len, bucketkey.dec_len, dim, num_labels, dropout)\n",
    "    return generate_lstm_sym\n",
    "\n",
    "def perplexity(label, pred, ignore_label):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        if label[i] == ignore_label:\n",
    "            break\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "train_iter = EncoderDecoderIter(\n",
    "    data_label, word2idx, idx2word, num_hidden, num_layers, \n",
    "    get_lstm_init_states, batch_size=1, num_buckets=num_buckets, shuffle=shuffle, rev=rev\n",
    ")\n",
    "\n",
    "batch_size = train_iter.batch_size\n",
    "print('batch_size:', batch_size)\n",
    "# load parameters file if exists!!!\n",
    "\n",
    "model_args = {}\n",
    "if os.path.isfile('%s/%s-symbol.json' % (params_dir, expt_name)):\n",
    "    filelist = os.listdir(params_dir) # get list of params file\n",
    "    paramfilelist = []\n",
    "    for f in filelist:\n",
    "        if f.startswith('%s-' % expt_name) and f.endswith('.params'):\n",
    "            paramfilelist.append( int(re.split(r'[-.]', f)[1]) )\n",
    "    last_iteration = max(paramfilelist)\n",
    "    print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "    tmp = mx.model.FeedForward.load('%s/%s' % (params_dir, expt_name), last_iteration)\n",
    "    model_args.update({\n",
    "        'arg_params' : tmp.arg_params,\n",
    "        'aux_params' : tmp.aux_params,\n",
    "        'begin_epoch' : tmp.begin_epoch\n",
    "    })\n",
    "\n",
    "model = mx.model.FeedForward(\n",
    "    ctx           = context, # uses all the available CPU in the machine\n",
    "    symbol        = get_lstm_sym_generator(num_layers, num_hidden, num_labels, dropout),\n",
    "    num_epoch     = iterations,\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.0,\n",
    "    wd            = 0.00001,\n",
    "    initializer   = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "    **model_args\n",
    ")\n",
    "\n",
    "if not os.path.exists(params_dir):\n",
    "    os.makedirs(params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric = mx.metric.np(perplexity, use_ignore=True, ignore_label=num_labels),\n",
    "    batch_end_callback = [ mx.callback.Speedometer(batch_size, frequent=100) ],\n",
    "    epoch_end_callback = [ mx.callback.do_checkpoint( '%s/%s' % (params_dir, expt_name) ) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = mx.model.FeedForward.load('%s/%s' % ('params', 'july24'), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.arg_params['embed_weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx['<PAD>']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
