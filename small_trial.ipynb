{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def perplexity(label, pred, ignore_label):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        if label[i] == ignore_label:\n",
    "            break\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enc_train_input='data/1.en'\n",
    "# dec_train_input='data/1.ru'\n",
    "enc_train_input='data/train.en'\n",
    "dec_train_input='data/train.ru'\n",
    "\n",
    "num_buckets=1\n",
    "num_layers=3\n",
    "num_hidden=10\n",
    "batch_size=1\n",
    "iterations=1\n",
    "expt_name='simple'\n",
    "params_dir='params'\n",
    "shuffle=False\n",
    "reverse=True\n",
    "top_words=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# import numpy as np\n",
    "\n",
    "remove_chars = re.compile(r'([\\(\\)\\'\\\"{}\\[\\]\\*\\-/])')\n",
    "punctuation = re.compile(r'([!,.?:;@#$%&]+)')\n",
    "def filter_text(text):\n",
    "    text = remove_chars.sub(' ', text)\n",
    "    # returns the text without the <EOS> token\n",
    "    text = punctuation.sub(r' \\1 ', text) # replaces the punctuation so that there is a space seperating it from the word\n",
    "    text = text.lower().strip(' \\t\\n') # replaces big caps with small caps\n",
    "    return text\n",
    "\n",
    "white_spaces = re.compile(r'[ \\n\\r\\t]+')\n",
    "def get_vocab(file, vocab_count={}):\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            tokens = white_spaces.split(filter_text(line))\n",
    "            for token in tokens:\n",
    "                if len(token) > 0:\n",
    "                    if token in vocab_count:\n",
    "                        vocab_count[token] += 1\n",
    "                    else:\n",
    "                        vocab_count[token] = 1\n",
    "    return vocab_count\n",
    "\n",
    "def text_2_indices(word2idx, text):\n",
    "    # return the list of indices representing this text including the <EOS> token at the end...\n",
    "    tokens = white_spaces.split(filter_text(text))\n",
    "    indices = []\n",
    "    unk_index = word2idx.get('<UNK>')\n",
    "    indices = [ word2idx.get(token, unk_index) for token in tokens ]\n",
    "    indices.append(word2idx['<EOS>'])\n",
    "    return np.array(indices)\n",
    "\n",
    "def get_unified_vocab(enc_input_file, dec_input_file, percentile=80):\n",
    "    vocab_count = get_vocab(enc_input_file) # this returns a dictionary\n",
    "    vocab_count = get_vocab(dec_input_file, vocab_count) # this returns a dictionary\n",
    "    \n",
    "    word_distribution = np.array( [ v for v in vocab_count.values() ] )\n",
    "    min_count = np.percentile(word_distribution, percentile)\n",
    "    vocab = []\n",
    "    for k,v in vocab_count.items():\n",
    "        if v >= min_count:\n",
    "            vocab.append(k)\n",
    "    vocab.sort()\n",
    "    \n",
    "    vocab.append('<UNK>') # token representing a word unseen in the training set, reserved for rare words\n",
    "    vocab.append('<EOS>') # token representing the End-of-Sentence\n",
    "    vocab.append('<PAD>') # token representing the padding for use in bucketing RNN of different lengths\n",
    "    \n",
    "    word2idx = { w:i for i,w in enumerate(vocab) }\n",
    "    idx2word = [ w for w in vocab ]\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def get_data_label(enc_input_file, dec_input_file, word2idx):\n",
    "    enc_input = []\n",
    "    with open(enc_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            enc_input.append(indices)\n",
    "\n",
    "    dec_input = []\n",
    "    with open(dec_input_file, 'r', encoding='utf-8', errors='ignore') as fid:\n",
    "        for line in fid:\n",
    "            indices = text_2_indices(word2idx, line)\n",
    "            dec_input.append(indices)\n",
    "    return np.array( list(zip(enc_input, dec_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdf ssd !!!'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_text('asdf/ssd!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_unified_vocab(enc_train_input, dec_train_input, top_words)\n",
    "train_data_label = get_data_label(enc_train_input, dec_train_input, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!!',\n",
       " '!!!',\n",
       " '!&',\n",
       " '!,',\n",
       " '!;',\n",
       " '!?',\n",
       " '#',\n",
       " '$',\n",
       " '$,',\n",
       " '$.',\n",
       " '$:',\n",
       " '%',\n",
       " '%!',\n",
       " '%,',\n",
       " '%.',\n",
       " '%.&#',\n",
       " '%:',\n",
       " '%;',\n",
       " '%?',\n",
       " '&',\n",
       " '&#',\n",
       " '+',\n",
       " '+1',\n",
       " '+2',\n",
       " '+20',\n",
       " '+3',\n",
       " '+39',\n",
       " ',',\n",
       " ',&',\n",
       " ',&#',\n",
       " ',,',\n",
       " ',.',\n",
       " '.',\n",
       " '.&',\n",
       " '.&#',\n",
       " '.,',\n",
       " '.,..',\n",
       " '..',\n",
       " '...',\n",
       " '...,',\n",
       " '....',\n",
       " '.....',\n",
       " '.:',\n",
       " '.;',\n",
       " '.?',\n",
       " '0',\n",
       " '00',\n",
       " '000',\n",
       " '000jihadi',\n",
       " '000th',\n",
       " '000\\xa0children',\n",
       " '000\\xa0kilometers',\n",
       " '000”',\n",
       " '007',\n",
       " '01',\n",
       " '010',\n",
       " '011630',\n",
       " '0116302248',\n",
       " '012',\n",
       " '018',\n",
       " '02',\n",
       " '025',\n",
       " '029',\n",
       " '03',\n",
       " '033',\n",
       " '04',\n",
       " '05',\n",
       " '057',\n",
       " '06',\n",
       " '07',\n",
       " '070',\n",
       " '0711',\n",
       " '08',\n",
       " '08»',\n",
       " '09',\n",
       " '0\\xa0баллов',\n",
       " '0°с',\n",
       " '0»',\n",
       " '0”',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '100amp',\n",
       " '100bn',\n",
       " '100m',\n",
       " '100th',\n",
       " '100\\xa0000',\n",
       " '100\\xa0млрд',\n",
       " '101',\n",
       " '101000',\n",
       " '1010010010001',\n",
       " '101st',\n",
       " '102',\n",
       " '1029',\n",
       " '103',\n",
       " '10307',\n",
       " '104',\n",
       " '105',\n",
       " '10500',\n",
       " '105m',\n",
       " '105th',\n",
       " '106',\n",
       " '106000',\n",
       " '107',\n",
       " '108',\n",
       " '1084',\n",
       " '109',\n",
       " '1090',\n",
       " '10amp',\n",
       " '10over100',\n",
       " '10th',\n",
       " '10\\xa0',\n",
       " '10\\xa0000',\n",
       " '10\\xa0000\\xa0',\n",
       " '10\\xa0800',\n",
       " '10\\xa0°c',\n",
       " '10\\xa0лет',\n",
       " '10°c',\n",
       " '10»',\n",
       " '10с',\n",
       " '10–12',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000',\n",
       " '110th',\n",
       " '111',\n",
       " '1110',\n",
       " '1111',\n",
       " '1118',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '1150',\n",
       " '11500',\n",
       " '115000',\n",
       " '115th',\n",
       " '116',\n",
       " '117',\n",
       " '117\\xa0000',\n",
       " '118',\n",
       " '119',\n",
       " '119\\xa0000',\n",
       " '11th',\n",
       " '11\\xa0000',\n",
       " '11\\xa0730',\n",
       " '11°c',\n",
       " '11»',\n",
       " '11сентября',\n",
       " '11”',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '120000',\n",
       " '1208',\n",
       " '120\\xa0000',\n",
       " '121',\n",
       " '1213',\n",
       " '122',\n",
       " '1224',\n",
       " '123',\n",
       " '124',\n",
       " '1244',\n",
       " '124th',\n",
       " '125',\n",
       " '1250',\n",
       " '125000',\n",
       " '126',\n",
       " '12600',\n",
       " '126th',\n",
       " '127',\n",
       " '127000',\n",
       " '1272',\n",
       " '128',\n",
       " '1284',\n",
       " '1291',\n",
       " '1293',\n",
       " '12th',\n",
       " '12\\xa0000',\n",
       " '12\\xa0nautical',\n",
       " '12\\xa0морских',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '13000',\n",
       " '130j',\n",
       " '130mm',\n",
       " '130\\xa0000',\n",
       " '131',\n",
       " '132',\n",
       " '13240',\n",
       " '1325',\n",
       " '133',\n",
       " '133000',\n",
       " '13320',\n",
       " '134',\n",
       " '13444',\n",
       " '134th',\n",
       " '135',\n",
       " '1350',\n",
       " '13500',\n",
       " '136',\n",
       " '1368',\n",
       " '1368—1644',\n",
       " '137',\n",
       " '1373',\n",
       " '138',\n",
       " '1382',\n",
       " '1389',\n",
       " '139',\n",
       " '13th',\n",
       " '13\\xa0000',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '14000',\n",
       " '1403',\n",
       " '140m',\n",
       " '140yen',\n",
       " '141',\n",
       " '142',\n",
       " '142000',\n",
       " '1424',\n",
       " '142nd',\n",
       " '143',\n",
       " '1430',\n",
       " '1431',\n",
       " '143rd',\n",
       " '144',\n",
       " '1441',\n",
       " '1447',\n",
       " '1448',\n",
       " '145',\n",
       " '1450',\n",
       " '14510',\n",
       " '1453',\n",
       " '146',\n",
       " '1465',\n",
       " '146th',\n",
       " '147',\n",
       " '1477',\n",
       " '148',\n",
       " '1485',\n",
       " '149',\n",
       " '1492',\n",
       " '149840',\n",
       " '14th',\n",
       " '14\\xa0',\n",
       " '14»',\n",
       " '14”',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '150000',\n",
       " '1501',\n",
       " '150\\xa0000',\n",
       " '151',\n",
       " '152',\n",
       " '1520',\n",
       " '1520’s',\n",
       " '1528',\n",
       " '153',\n",
       " '154',\n",
       " '15455',\n",
       " '1546',\n",
       " '155',\n",
       " '1550',\n",
       " '1559',\n",
       " '156',\n",
       " '1564',\n",
       " '1568',\n",
       " '156th',\n",
       " '157',\n",
       " '158',\n",
       " '1585',\n",
       " '159',\n",
       " '15sg',\n",
       " '15th',\n",
       " '15\\xa0000',\n",
       " '15\\xa0000\\xa0',\n",
       " '15\\xa0800',\n",
       " '15\\xa0°c',\n",
       " '15°c',\n",
       " '15’s',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '16000',\n",
       " '160000',\n",
       " '1607',\n",
       " '161',\n",
       " '1612',\n",
       " '161st',\n",
       " '162',\n",
       " '1623',\n",
       " '1624',\n",
       " '1627',\n",
       " '163',\n",
       " '1630',\n",
       " '1630’s',\n",
       " '1636',\n",
       " '163\\xa0000',\n",
       " '164',\n",
       " '1644',\n",
       " '1648',\n",
       " '1649',\n",
       " '165',\n",
       " '165000',\n",
       " '165\\xa0000',\n",
       " '166',\n",
       " '1665',\n",
       " '1668',\n",
       " '167',\n",
       " '1673',\n",
       " '168',\n",
       " '1689',\n",
       " '169',\n",
       " '1694',\n",
       " '169th',\n",
       " '16s',\n",
       " '16th',\n",
       " '16\\xa0000',\n",
       " '16–17',\n",
       " '16’s',\n",
       " '17',\n",
       " '17+',\n",
       " '17+»',\n",
       " '170',\n",
       " '1700',\n",
       " '17000',\n",
       " '1700s',\n",
       " '1700е',\n",
       " '1700’s',\n",
       " '1701',\n",
       " '1705',\n",
       " '1707',\n",
       " '171',\n",
       " '1711',\n",
       " '171500',\n",
       " '1718',\n",
       " '1719',\n",
       " '172',\n",
       " '1724',\n",
       " '1727',\n",
       " '173',\n",
       " '1732',\n",
       " '174',\n",
       " '1746',\n",
       " '175',\n",
       " '1750',\n",
       " '1755',\n",
       " '1756',\n",
       " '175th',\n",
       " '176',\n",
       " '1764',\n",
       " '1766000',\n",
       " '1769',\n",
       " '177',\n",
       " '1770',\n",
       " '1771',\n",
       " '1776',\n",
       " '178',\n",
       " '1780',\n",
       " '1780’s',\n",
       " '1783',\n",
       " '1784',\n",
       " '1785',\n",
       " '1787',\n",
       " '1789',\n",
       " '179',\n",
       " '1790’s',\n",
       " '1795',\n",
       " '1798',\n",
       " '17bn',\n",
       " '17th',\n",
       " '17\\xa0010',\n",
       " '17\\xa0revolution',\n",
       " '17»',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '18000',\n",
       " '180000',\n",
       " '1800s',\n",
       " '1800’s',\n",
       " '1802',\n",
       " '1804',\n",
       " '1807',\n",
       " '1809',\n",
       " '180\\xa0hours',\n",
       " '180°',\n",
       " '181',\n",
       " '1810',\n",
       " '1812',\n",
       " '1815',\n",
       " '1816',\n",
       " '1817',\n",
       " '1818',\n",
       " '1819',\n",
       " '182',\n",
       " '1820',\n",
       " '1820’s',\n",
       " '1821',\n",
       " '1825',\n",
       " '1826',\n",
       " '1827',\n",
       " '182nd',\n",
       " '183',\n",
       " '1830',\n",
       " '1830s',\n",
       " '1830’s',\n",
       " '1832',\n",
       " '1836',\n",
       " '1837',\n",
       " '1839',\n",
       " '1839�42',\n",
       " '184',\n",
       " '1840',\n",
       " '1840’s',\n",
       " '1841',\n",
       " '1842',\n",
       " '1844',\n",
       " '1845',\n",
       " '1846',\n",
       " '1847',\n",
       " '1848',\n",
       " '185',\n",
       " '1850',\n",
       " '1850’s',\n",
       " '1853',\n",
       " '1854',\n",
       " '1856',\n",
       " '1857',\n",
       " '1859',\n",
       " '186',\n",
       " '1860',\n",
       " '1860s',\n",
       " '1860’s',\n",
       " '1861',\n",
       " '1862',\n",
       " '1863',\n",
       " '1865',\n",
       " '1866',\n",
       " '1867',\n",
       " '1868',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '1870’s',\n",
       " '1871',\n",
       " '1873',\n",
       " '1874',\n",
       " '1875',\n",
       " '1877',\n",
       " '1878',\n",
       " '1879',\n",
       " '188',\n",
       " '1880',\n",
       " '1880’s',\n",
       " '1881',\n",
       " '1882',\n",
       " '1883',\n",
       " '1884',\n",
       " '1885',\n",
       " '1886',\n",
       " '1887',\n",
       " '1888',\n",
       " '1889',\n",
       " '189',\n",
       " '1890',\n",
       " '1890’s',\n",
       " '1891',\n",
       " '1892',\n",
       " '1893',\n",
       " '1894',\n",
       " '18945',\n",
       " '1895',\n",
       " '1896',\n",
       " '1897',\n",
       " '1898',\n",
       " '1899',\n",
       " '18th',\n",
       " '18\\xa0000',\n",
       " '18\\xa0300',\n",
       " '18’s',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '19000',\n",
       " '1900’s',\n",
       " '1901',\n",
       " '1902',\n",
       " '1903',\n",
       " '1904',\n",
       " '1905',\n",
       " '1906',\n",
       " '1907',\n",
       " '1908',\n",
       " '1909',\n",
       " '191',\n",
       " '1910',\n",
       " '1910’s',\n",
       " '1911',\n",
       " '1912',\n",
       " '1913',\n",
       " '1914',\n",
       " '1914г',\n",
       " '1914”',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '1919',\n",
       " '192',\n",
       " '1920',\n",
       " '1920s',\n",
       " '1920’s',\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '1929г',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1930гг',\n",
       " '1930х',\n",
       " '1930’s',\n",
       " '1931',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '1936–1939',\n",
       " '1937',\n",
       " '1938',\n",
       " '1938г',\n",
       " '1939',\n",
       " '194',\n",
       " '1940',\n",
       " '194000',\n",
       " '1940s',\n",
       " '1940гг',\n",
       " '1940х',\n",
       " '1940’s',\n",
       " '1941',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1944г',\n",
       " '1945',\n",
       " '1945г',\n",
       " '1946',\n",
       " '1947',\n",
       " '1947–1948',\n",
       " '1948',\n",
       " '1948г',\n",
       " '1949',\n",
       " '195',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1950г',\n",
       " '1950х',\n",
       " '1950‑1980',\n",
       " '1950’s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1953г',\n",
       " '1954',\n",
       " '1955',\n",
       " '19555',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1960s’',\n",
       " '1960»',\n",
       " '1960г',\n",
       " '1960х',\n",
       " '1960’s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1965»',\n",
       " '1966',\n",
       " '1966�76',\n",
       " '1967',\n",
       " '1967г',\n",
       " '1968',\n",
       " '1969',\n",
       " '197',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1970г',\n",
       " '1970гг',\n",
       " '1970х',\n",
       " '1970–е',\n",
       " '1970’s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1973г',\n",
       " '1974',\n",
       " '1975',\n",
       " '1975г',\n",
       " '1975гг',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1979г',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1980г',\n",
       " '1980х',\n",
       " '1980‑е',\n",
       " '1980’s',\n",
       " '1981',\n",
       " '1981г',\n",
       " '1982',\n",
       " '1983',\n",
       " '1983г',\n",
       " '1984',\n",
       " '1984»',\n",
       " '1984г',\n",
       " '1984гг',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1988–1994',\n",
       " '1988–1995',\n",
       " '1989',\n",
       " '1989\\xa0г',\n",
       " '1989г',\n",
       " '1989–1997',\n",
       " '1989’s',\n",
       " '199',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1990г',\n",
       " '1990ми',\n",
       " '1990х',\n",
       " '1990’s',\n",
       " '1991',\n",
       " '1991г',\n",
       " '1991года',\n",
       " '1991годов',\n",
       " '1991–1998',\n",
       " '1992',\n",
       " '1992»',\n",
       " '1992г',\n",
       " '1993',\n",
       " '1993–1997',\n",
       " '1994',\n",
       " '1994г',\n",
       " '1994–1995',\n",
       " '1994–2000',\n",
       " '1995',\n",
       " '1995–1999',\n",
       " '1996',\n",
       " '19968',\n",
       " '1996\\xa0гг',\n",
       " '1996г',\n",
       " '1997',\n",
       " '1997\\xa0',\n",
       " '1997г',\n",
       " '1997гг',\n",
       " '1997го',\n",
       " '1997–98',\n",
       " '1998',\n",
       " '1998was',\n",
       " '1998г',\n",
       " '1998гг',\n",
       " '1998–1999',\n",
       " '1999',\n",
       " '1999–2001',\n",
       " '19th',\n",
       " '19\\xa0000',\n",
       " '1billion',\n",
       " '1dollar',\n",
       " '1million',\n",
       " '1st',\n",
       " '1\\xa0000',\n",
       " '1\\xa0070',\n",
       " '1\\xa0240\\xa0000',\n",
       " '1\\xa0300',\n",
       " '1\\xa0324',\n",
       " '1\\xa0429',\n",
       " '1\\xa0491',\n",
       " '1°c',\n",
       " '1»',\n",
       " '1¼',\n",
       " '1”',\n",
       " '2',\n",
       " '2+2ampquot',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '20005',\n",
       " '2000made',\n",
       " '2000s',\n",
       " '2000succeeded',\n",
       " '2000г',\n",
       " '2000м',\n",
       " '2000ми',\n",
       " '2000–2004',\n",
       " '2000’s',\n",
       " '2000”',\n",
       " '2001',\n",
       " '2001and',\n",
       " '2001\\xa0г',\n",
       " '2001г',\n",
       " '2001гг',\n",
       " '2001года',\n",
       " '2002',\n",
       " '2002arab',\n",
       " '2002г',\n",
       " '2002–2003',\n",
       " '2003',\n",
       " '2003ampnbsp',\n",
       " '2003г',\n",
       " '2003–2004',\n",
       " '2003–2007',\n",
       " '2004',\n",
       " '2004african',\n",
       " '2004\\xa0гг',\n",
       " '2004»',\n",
       " '2004г',\n",
       " '2005',\n",
       " '2005\\xa0г',\n",
       " '2005»',\n",
       " '2005г',\n",
       " '2005гг',\n",
       " '2005–2006',\n",
       " '2006',\n",
       " '2006ampnbsp',\n",
       " '2006»',\n",
       " '2006г',\n",
       " '2007',\n",
       " '2007\\xa0г',\n",
       " '2007г',\n",
       " '2007�гг',\n",
       " '2008',\n",
       " '2008»',\n",
       " '2008г',\n",
       " '2008гг',\n",
       " '2008–2012',\n",
       " '2009',\n",
       " '2009\\xa0resolution',\n",
       " '2009г',\n",
       " '2009год',\n",
       " '200\\xa0000',\n",
       " '200\\xa0meters',\n",
       " '200\\xa0nautical',\n",
       " '200\\xa0морских',\n",
       " '200�000',\n",
       " '201',\n",
       " '2010',\n",
       " '20103',\n",
       " '2010\\xa0г',\n",
       " '2010г',\n",
       " '2010’s',\n",
       " '2010”',\n",
       " '2011',\n",
       " '2011»',\n",
       " '2011г',\n",
       " '2011’s',\n",
       " '2012',\n",
       " '2012г',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2020s',\n",
       " '2020\\xa0г',\n",
       " '2020»',\n",
       " '2020’s',\n",
       " '2020”',\n",
       " '2022',\n",
       " '2023',\n",
       " '2024',\n",
       " '2025',\n",
       " '2027',\n",
       " '2028',\n",
       " '2029',\n",
       " '203',\n",
       " '2030',\n",
       " '2030\\xa0г',\n",
       " '2030»',\n",
       " '2034',\n",
       " '2035',\n",
       " '2036',\n",
       " '204',\n",
       " '2040',\n",
       " '2040’s',\n",
       " '2042',\n",
       " '205',\n",
       " '2050',\n",
       " '2050’s',\n",
       " '2050”',\n",
       " '206',\n",
       " '2060',\n",
       " '2070',\n",
       " '2075',\n",
       " '208',\n",
       " '2080',\n",
       " '2080г',\n",
       " '2081',\n",
       " '2081г',\n",
       " '2085',\n",
       " '209',\n",
       " '20m',\n",
       " '20th',\n",
       " '20\\xa0',\n",
       " '20\\xa0000',\n",
       " '20\\xa0years',\n",
       " '20\\xa0лет',\n",
       " '20»',\n",
       " '20с',\n",
       " '20х',\n",
       " '20’s',\n",
       " '20”',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '21000',\n",
       " '2102',\n",
       " '2106',\n",
       " '211',\n",
       " '2116',\n",
       " '2118',\n",
       " '212',\n",
       " '2127',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '2156',\n",
       " '216',\n",
       " '217',\n",
       " '2172',\n",
       " '21870',\n",
       " '219',\n",
       " '21st',\n",
       " '21»',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '22000',\n",
       " '220000',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '225000',\n",
       " '226',\n",
       " '227',\n",
       " '2275',\n",
       " '229',\n",
       " '2293',\n",
       " '22nd',\n",
       " '22°c',\n",
       " '22»',\n",
       " '23',\n",
       " '230',\n",
       " '2300',\n",
       " '230th',\n",
       " '23200',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '238',\n",
       " '239',\n",
       " '23andme',\n",
       " '23\\xa0000',\n",
       " '23\\xa0years',\n",
       " '24',\n",
       " '240',\n",
       " '2400',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '25000',\n",
       " '250000',\n",
       " '2509',\n",
       " '250\\xa0000',\n",
       " '251',\n",
       " '252',\n",
       " '2520',\n",
       " '253',\n",
       " '254',\n",
       " '254840',\n",
       " '255',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '25th',\n",
       " '25\\xa0000',\n",
       " '25\\xa0years',\n",
       " '25\\xa0лет',\n",
       " '25мвт',\n",
       " '25с',\n",
       " '26',\n",
       " '260',\n",
       " '26000',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '2640',\n",
       " '265',\n",
       " '266',\n",
       " '267',\n",
       " '269',\n",
       " '26th',\n",
       " '26\\xa0000',\n",
       " '27',\n",
       " '270',\n",
       " '2700',\n",
       " '270\\xa0000',\n",
       " '272',\n",
       " '274',\n",
       " '275',\n",
       " '27500',\n",
       " '277',\n",
       " '27c',\n",
       " '27sk’s',\n",
       " '27th',\n",
       " '27\\xa0eu',\n",
       " '27°c',\n",
       " '27ск',\n",
       " '28',\n",
       " '280',\n",
       " '28000',\n",
       " '281',\n",
       " '283',\n",
       " '284',\n",
       " '286',\n",
       " '288',\n",
       " '289',\n",
       " '28th',\n",
       " '29',\n",
       " '290',\n",
       " '2900',\n",
       " '292',\n",
       " '293',\n",
       " '295',\n",
       " '296',\n",
       " '299',\n",
       " '29th',\n",
       " '2nd',\n",
       " '2oc',\n",
       " '2oc»',\n",
       " '2trillion',\n",
       " '2\\xa0',\n",
       " '2\\xa0000',\n",
       " '2\\xa0400',\n",
       " '2\\xa0billion',\n",
       " '2\\xa0million',\n",
       " '2\\xa0миллиарда',\n",
       " '2\\xa0миллионов',\n",
       " '2\\xa0трлн',\n",
       " '2°',\n",
       " ...]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "LSTMState = namedtuple(\"LSTMState\", [\"c\", \"h\"])\n",
    "LSTMParam = namedtuple(\"LSTMParam\", [\"i2h_weight\", \"i2h_bias\", \"h2h_weight\", \"h2h_bias\"])\n",
    "\n",
    "def lstm_cell(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0.):\n",
    "    \"\"\"LSTM Cell symbol\"\"\"\n",
    "    if dropout > 0.:\n",
    "        indata = mx.sym.Dropout(data=indata, p=dropout)\n",
    "    \n",
    "    i2h = mx.sym.FullyConnected(\n",
    "        data=indata,\n",
    "        weight=param.i2h_weight,\n",
    "        bias=param.i2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_i2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    h2h = mx.sym.FullyConnected(\n",
    "        data=prev_state.h,\n",
    "        weight=param.h2h_weight,\n",
    "        bias=param.h2h_bias,\n",
    "        num_hidden=num_hidden * 4,\n",
    "        name=\"t%d_l%d_h2h\" % (seqidx, layeridx)\n",
    "    )\n",
    "\n",
    "    gates = i2h + h2h\n",
    "    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4, name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n",
    "\n",
    "    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n",
    "    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n",
    "    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n",
    "    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n",
    "\n",
    "    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n",
    "    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n",
    "\n",
    "    return LSTMState(c=next_c, h=next_h)\n",
    "\n",
    "def init_lstm(num_layer):\n",
    "    param_cells = []\n",
    "    last_states = []\n",
    "    for i in range(num_layer):\n",
    "        param_cells.append(\n",
    "            LSTMParam(\n",
    "                i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n",
    "                i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n",
    "                h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n",
    "                h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)\n",
    "            )\n",
    "        )\n",
    "        last_states.append(\n",
    "            LSTMState(\n",
    "                c=mx.sym.Variable(\"l%d_init_c\" % i),\n",
    "                h=mx.sym.Variable(\"l%d_init_h\" % i)\n",
    "            )\n",
    "        )\n",
    "    return param_cells, last_states\n",
    "\n",
    "def lstm_unroll(num_layer, seqlen, num_hidden, num_labels, dropout=0.0):\n",
    "    cls_weight   = mx.sym.Variable(\"cls_weight\")\n",
    "    cls_bias     = mx.sym.Variable(\"cls_bias\")\n",
    "    embed_weight = mx.sym.Variable(\"embed_weight\")\n",
    "\n",
    "    param_cells, last_states = init_lstm(num_layer)\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('label')\n",
    "    \n",
    "    embed = mx.sym.Embedding(\n",
    "        data=data, # the idx to the embedding\n",
    "        input_dim=num_labels, # the number of rows for embed_weight\n",
    "        weight=embed_weight,  # the matrix representing the idx2vec\n",
    "        output_dim=num_hidden, # the number of cols for embed_weight\n",
    "        name='embed'\n",
    "    )\n",
    "    \n",
    "    wordvec = mx.sym.SliceChannel(data=embed, num_outputs=seqlen, squeeze_axis=1)\n",
    "\n",
    "    hidden_all = []\n",
    "    for seqidx in range(seqlen):\n",
    "        hidden = wordvec[seqidx]\n",
    "        # stack LSTM\n",
    "        for i in range(num_layer):\n",
    "            dp = 0.0 if i == 0 else dropout\n",
    "            next_state = lstm_cell(\n",
    "                num_hidden,\n",
    "                indata=hidden,\n",
    "                prev_state=last_states[i],\n",
    "                param=param_cells[i],\n",
    "                seqidx=seqidx,\n",
    "                layeridx=i,\n",
    "                dropout=dp\n",
    "            )\n",
    "            hidden = next_state.h\n",
    "            last_states[i] = next_state\n",
    "        # decoder\n",
    "        if dropout > 0.0:\n",
    "            hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "        \n",
    "        hidden_all.append(hidden)\n",
    "        \n",
    "    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n",
    "    pred = mx.sym.FullyConnected(\n",
    "        data=hidden_concat,\n",
    "        num_hidden=num_labels, # num_labels is the index of <PAD> that means this layer will predict 0, 1, ..., num_labels-1\n",
    "        weight=cls_weight,\n",
    "        bias=cls_bias,\n",
    "        name='pred'\n",
    "    )\n",
    "\n",
    "    label = mx.sym.transpose(data=label) # e.g. if shape is (1,M) it becomes (M,1)\n",
    "    label = mx.sym.Reshape(data=label, shape=(-1,)) # if shape is (M,1) it becomes (M,)\n",
    "    output = mx.sym.SoftmaxOutput(\n",
    "        data=pred,\n",
    "        label=label,\n",
    "        name='t%d_softmax' % seqidx,\n",
    "        use_ignore=True,\n",
    "        ignore_label=num_labels # ignore the index of <PAD>\n",
    "    ) # output becomes (num_labels, M)\n",
    "    return output\n",
    "\n",
    "def get_lstm_sym_generator(num_layers, num_hidden, num_labels, dropout=0.0):\n",
    "    def generate_lstm_sym(seqlen):\n",
    "        return lstm_unroll(num_layers, seqlen, num_hidden, num_labels, dropout)\n",
    "    return generate_lstm_sym\n",
    "\n",
    "def get_lstm_init_states(num_layers, num_dim, batch_size=1):\n",
    "    init_h = [('l%d_init_h' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_c = [('l%d_init_c' % i, (batch_size, num_dim)) for i in range(num_layers)]\n",
    "    init_states = init_h + init_c\n",
    "    return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import mxnet as mx\n",
    "# import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class EncoderDecoderBatch(object):\n",
    "    def __init__(self, all_data, all_label, init_states, bucket_key):\n",
    "        self.pad = 0 # at this point i do not know what is this for...\n",
    "        \n",
    "        #all_data.shape is (x,y,z)\n",
    "        self.batch_size = all_data.shape[0]\n",
    "        \n",
    "        # provide data, essential assignment\n",
    "        self.data = [ mx.nd.array(all_data) ]\n",
    "        \n",
    "        # essential assignment\n",
    "        self.provide_data = [('data', (self.batch_size, bucket_key))]\n",
    "        for x in init_states:\n",
    "            self.data.append(mx.nd.zeros(x[1])) # x[1] is the shape of the initial data\n",
    "            self.provide_data.append(x)\n",
    "\n",
    "        # provide label, essential assignment\n",
    "        self.label = [ mx.nd.array(all_label) ]\n",
    "        self.provide_label = [ ('label', (self.batch_size, bucket_key)) ]\n",
    "        \n",
    "        self.init_states = init_states\n",
    "\n",
    "        # bucket_key is essential for this databatch\n",
    "        self.bucket_key = bucket_key\n",
    "\n",
    "def synchronize_batch_size(train_iter, test_iter):\n",
    "    batch_size = min(train_iter.batch_size, test_iter.batch_size)\n",
    "    train_iter.batch_size = batch_size\n",
    "    test_iter.batch_size = batch_size\n",
    "    train_iter.generate_init_states()\n",
    "    test_iter.generate_init_states()\n",
    "\n",
    "# now define the bucketing, padding and batching SequenceIterator...\n",
    "class EncoderDecoderIter(mx.io.DataIter):\n",
    "    def __init__(self, data_label, word2idx, idx2word, num_hidden, num_layers,\n",
    "                 init_states_function, batch_size=1, num_buckets=10, shuffle=False, rev=False):\n",
    "\n",
    "        super(EncoderDecoderIter, self).__init__() # calling DataIter.__init__()\n",
    "\n",
    "        # data is a numpy array of 3 dimensions, #, timesteps, vector_dim\n",
    "        self.data_label = data_label\n",
    "\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "        # now we need to find the buckets based on the input data...\n",
    "        self.buckets, self.buckets_count, self.assignments = self.generate_buckets()\n",
    "        # buckets are a tuple of the encoder/decoder length\n",
    "\n",
    "        self.batch_size = min(np.min(self.buckets_count), batch_size)\n",
    "        self.init_states_function = init_states_function\n",
    "        self.pad_label = word2idx['<PAD>']\n",
    "        self.shuffle = shuffle\n",
    "        self.rev = rev # reverse the encoder input\n",
    "        self.reset()\n",
    "        self.generate_init_states()\n",
    "\n",
    "    def generate_init_states(self):\n",
    "        self.init_states = self.init_states_function(self.num_layers, self.num_hidden, self.batch_size)\n",
    "\n",
    "    def generate_buckets(self):\n",
    "        enc_dec_data = [ len(data)+len(label)-1 for data, label in self.data_label ]\n",
    "        enc_dec_data = np.reshape(np.array(enc_dec_data), (-1, 1))\n",
    "\n",
    "        kmeans = KMeans(n_clusters=self.num_buckets, random_state=1) # use clustering to decide the buckets\n",
    "        assignments = kmeans.fit_predict(enc_dec_data) # get the assignments\n",
    "\n",
    "        # get the max of every cluster\n",
    "        buckets = np.array([np.amax(enc_dec_data[assignments==i]) for i in range(self.num_buckets)])\n",
    "\n",
    "        # get # of sequences in each bucket... then assign the batch size as the minimum(minimum(bucketsize), batchsize)\n",
    "        buckets_count = np.array([enc_dec_data[assignments==i].shape[0] for i in range(self.num_buckets)])\n",
    "\n",
    "        return buckets, buckets_count, assignments\n",
    "\n",
    "    @property\n",
    "    def default_bucket_key(self):\n",
    "        return np.amax(self.buckets)\n",
    "\n",
    "    @property\n",
    "    def provide_data(self): # this is necessary when specifying custom DataIter\n",
    "        # length of data variable is length of encoder + length of decoder\n",
    "        bucket_key = self.default_bucket_key\n",
    "        return [('data', (self.batch_size, bucket_key))] + self.init_states\n",
    "    #\n",
    "    @property\n",
    "    def provide_label(self): # this is necessary when specifying custom DataIter\n",
    "        # length of label variable is only the length of decoder\n",
    "        bucket_key = self.default_bucket_key\n",
    "        return [('label', (self.batch_size, bucket_key))]\n",
    "\n",
    "    # for custom DataIter, we must implement this class as an iterable and return a DataBatch\n",
    "    def __iter__(self): # this is necessary to convert this class into an iterable\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter_next():\n",
    "            # suppose to get self.cursor:self.cursor + self.batch_size\n",
    "            batch = self.data_label[self.assignments == self.cur_permute_bucket]\\\n",
    "                [self.in_bucket_permutation[self.cursor:self.cursor+self.batch_size]]\n",
    "\n",
    "            # get size of this bucket\n",
    "            seqlen = self.buckets[self.cur_permute_bucket] # this seqlen already deducted the <EOS>\n",
    "\n",
    "            all_data = np.full((self.batch_size, seqlen), self.pad_label, dtype=float)\n",
    "            all_label = np.full((self.batch_size, seqlen), self.pad_label, dtype=float)\n",
    "\n",
    "            for i, (data, label) in enumerate(batch):\n",
    "                if self.rev:\n",
    "                    # reverse the input except for the <EOS> at end of input\n",
    "                    # according to Ilya Sutskever et al. Sequence to Sequence Learning with Neural Networks\n",
    "                    # there is a reason for this... which you should ask freddy\n",
    "                    data[:-1] = np.flipud(data[:-1])\n",
    "\n",
    "                all_data[i, :data.shape[0]] = data\n",
    "                all_data[i, data.shape[0]:data.shape[0]+label.shape[0]-1] = label[:-1]\n",
    "                all_label[i, data.shape[0]-1:data.shape[0]-1+label.shape[0]] = label\n",
    "\n",
    "            return EncoderDecoderBatch(all_data, all_label, self.init_states, seqlen)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def iter_next(self):\n",
    "        self.cursor += self.batch_size\n",
    "        if self.cursor < self.buckets_count[self.cur_permute_bucket]:\n",
    "            if self.cursor + self.batch_size > self.buckets_count[self.cur_permute_bucket]:\n",
    "                # it is going to overflow the bucket\n",
    "                self.cursor -= self.cursor + self.batch_size - self.buckets_count[self.cur_permute_bucket]\n",
    "            return True\n",
    "        else:\n",
    "            self.cur_bucket += 1\n",
    "            if self.cur_bucket < self.num_buckets:\n",
    "                self.cursor = 0\n",
    "                self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "                if self.shuffle:\n",
    "                    self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "                else:\n",
    "                    self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def reset(self): # for iterable\n",
    "        self.cursor = -self.batch_size\n",
    "        self.cur_bucket = 0\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.bucket_permutation = np.random.permutation(self.num_buckets)\n",
    "        else:\n",
    "            self.bucket_permutation = np.array(range(self.num_buckets))\n",
    "\n",
    "        self.cur_permute_bucket = self.bucket_permutation[self.cur_bucket]\n",
    "        if self.shuffle:\n",
    "            self.in_bucket_permutation = np.random.permutation(self.buckets_count[self.cur_permute_bucket])\n",
    "        else:\n",
    "            self.in_bucket_permutation = np.array(range(self.buckets_count[self.cur_permute_bucket]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_iter = EncoderDecoderIter(train_data_label, word2idx, idx2word,\n",
    "            num_hidden, num_layers, get_lstm_init_states, batch_size=batch_size,\n",
    "            num_buckets=num_buckets, shuffle=shuffle, rev=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_iter(iter):\n",
    "    iter.reset()\n",
    "    print('provide_data: ', iter.provide_data)\n",
    "    print('provide_label: ', iter.provide_label)\n",
    "    print('buckets: ', iter.buckets)\n",
    "    print('buckets count: ', iter.buckets_count)\n",
    "    print('assignments: ', iter.assignments)\n",
    "    print('batch_size: ', iter.batch_size)\n",
    "    for i, data_batch in enumerate(iter):\n",
    "        print(i, data_batch.provide_data)\n",
    "        print(i, data_batch.provide_label)\n",
    "        print(i, data_batch.bucket_key)\n",
    "#         print(i, data_batch.data)\n",
    "        for j, d in enumerate(data_batch.data):\n",
    "#             print(i, j, data_batch.data[j].shape)\n",
    "            if j==0:\n",
    "                print(i, 'data:', data_batch.data[j].asnumpy())\n",
    "#         print(i, data_batch.label)\n",
    "#         print(i, data_batch.label[0].shape)\n",
    "        print(i, 'label:', data_batch.label[0].asnumpy())\n",
    "#         print('\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provide_data:  [('data', (1, 12)), ('l0_init_h', (1, 10)), ('l1_init_h', (1, 10)), ('l2_init_h', (1, 10)), ('l0_init_c', (1, 10)), ('l1_init_c', (1, 10)), ('l2_init_c', (1, 10))]\n",
      "provide_label:  [('label', (1, 12))]\n",
      "buckets:  [12]\n",
      "buckets count:  [10]\n",
      "assignments:  [0 0 0 0 0 0 0 0 0 0]\n",
      "batch_size:  1\n",
      "0 [('data', (1, 12)), ('l0_init_h', (1, 10)), ('l1_init_h', (1, 10)), ('l2_init_h', (1, 10)), ('l0_init_c', (1, 10)), ('l1_init_c', (1, 10)), ('l2_init_c', (1, 10))]\n",
      "0 [('label', (1, 12))]\n",
      "0 12\n",
      "0 data: [[  6.  41.  23.  42.  42.  42.  42.  42.  42.  42.  42.  42.]]\n",
      "0 label: [[ 42.  23.  41.  42.  42.  42.  42.  42.  42.  42.  42.  42.]]\n"
     ]
    }
   ],
   "source": [
    "print_iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model params/simple at epoch 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "context = mx.cpu()\n",
    "\n",
    "model_args = {}\n",
    "if os.path.isfile('%s/%s-symbol.json' % (params_dir, expt_name)):\n",
    "    filelist = os.listdir(params_dir) # get list of params file\n",
    "    paramfilelist = []\n",
    "    for f in filelist:\n",
    "        if f.startswith('%s-' % expt_name) and f.endswith('.params'):\n",
    "            paramfilelist.append( int(re.split(r'[-.]', f)[1]) )\n",
    "    last_iteration = max(paramfilelist)\n",
    "    print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "    tmp = mx.model.FeedForward.load('%s/%s' % (params_dir, expt_name), last_iteration)\n",
    "    model_args.update({\n",
    "        'arg_params' : tmp.arg_params,\n",
    "        'aux_params' : tmp.aux_params,\n",
    "        'begin_epoch' : tmp.begin_epoch\n",
    "    })\n",
    "\n",
    "num_labels = len(word2idx)\n",
    "iterations = 1000\n",
    "model = mx.model.FeedForward(\n",
    "    ctx           = context, # uses all the available CPU in the machine\n",
    "    symbol        = get_lstm_sym_generator(num_layers, num_hidden, num_labels),\n",
    "    num_epoch     = iterations,\n",
    "    learning_rate = 0.1,\n",
    "    momentum      = 0.0,\n",
    "    wd            = 0.00001,\n",
    "    initializer   = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "    **model_args\n",
    ")\n",
    "\n",
    "if not os.path.exists(params_dir):\n",
    "    os.makedirs(params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric = mx.metric.np(perplexity, use_ignore=True, ignore_label=num_labels),\n",
    "    batch_end_callback = [ mx.callback.Speedometer(batch_size, frequent=10) ],\n",
    "    epoch_end_callback = [ mx.callback.do_checkpoint( '%s/%s' % (params_dir, expt_name) ) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model params/simple at epoch 1000\n"
     ]
    }
   ],
   "source": [
    "last_iteration = iterations\n",
    "print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "_, arg_params, __ = mx.model.load_checkpoint('%s/%s' % (params_dir, expt_name), last_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_inference_symbol(num_layer, num_hidden, num_labels, dropout=0.0):\n",
    "    param_cells, last_states = init_lstm(num_layer)\n",
    "    \n",
    "    data = mx.sym.Variable('data')\n",
    "    embed_weight=mx.sym.Variable(\"embed_weight\")\n",
    "    \n",
    "    hidden = mx.sym.Embedding(data=data, input_dim=num_labels, weight=embed_weight, output_dim=num_hidden, name='embed')\n",
    "    \n",
    "    # stack layers of LSTM for 1 sequence\n",
    "    for i in range(num_layer):\n",
    "        dp = 0.0 if i == 0 else dropout\n",
    "        next_state = lstm_cell(\n",
    "            num_hidden,\n",
    "            indata=hidden,\n",
    "            prev_state=last_states[i],\n",
    "            param=param_cells[i],\n",
    "            seqidx=0,\n",
    "            layeridx=i,\n",
    "            dropout=dp\n",
    "        )\n",
    "        hidden = next_state.h\n",
    "        last_states[i] = next_state\n",
    "    \n",
    "    if dropout > 0.0:\n",
    "        hidden = mx.sym.Dropout(data=hidden, p=dropout)\n",
    "    \n",
    "    output = []\n",
    "    for state in last_states:\n",
    "        # very important to be in this order!!!\n",
    "        output.append(state.h)\n",
    "        output.append(state.c)\n",
    "    \n",
    "    return mx.sym.Group(output)\n",
    "\n",
    "class LSTMInferenceModel(object):\n",
    "    def __init__(self, num_layer, num_hidden, num_labels, arg_params, ctx=mx.cpu(), dropout=0.0):\n",
    "        \n",
    "        self.sym = lstm_inference_symbol(num_layer, num_hidden, num_labels, dropout)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        batch_size = 1\n",
    "        init_states = get_lstm_init_states(num_layer, num_hidden, batch_size)\n",
    "        data_shape = [(\"data\", (batch_size, ))]\n",
    "\n",
    "        input_shapes = dict(init_states + data_shape)\n",
    "        self.executor = self.sym.simple_bind(ctx=ctx, **input_shapes)\n",
    "\n",
    "        # copy the transition parameters over to executor\n",
    "        for key in self.executor.arg_dict.keys():\n",
    "            if key in arg_params:\n",
    "                arg_params[key].copyto(self.executor.arg_dict[key])\n",
    "\n",
    "        state_name = []\n",
    "        for i in range(num_layer):\n",
    "            # very important to be in this order!!!\n",
    "            state_name.append(\"l%d_init_h\" % i)\n",
    "            state_name.append(\"l%d_init_c\" % i)\n",
    "\n",
    "        self.states_dict = dict(zip(state_name, self.executor.outputs)) # this transfer the output of previous state to current\n",
    "\n",
    "        self.cls_weight = arg_params['cls_weight']\n",
    "        self.cls_bias   = arg_params['cls_bias']\n",
    "        self.ctx = ctx\n",
    "\n",
    "    def predict(self, x):\n",
    "        # another symbolic graph here... \n",
    "        data       = mx.sym.Variable('data')\n",
    "        cls_weight = mx.sym.Variable(\"cls_weight\")\n",
    "        cls_bias   = mx.sym.Variable(\"cls_bias\")\n",
    "    \n",
    "        pred = mx.sym.FullyConnected(\n",
    "            data       = data,\n",
    "            num_hidden = self.num_labels,\n",
    "            weight     = cls_weight,\n",
    "            bias       = cls_bias,\n",
    "            name       = 'pred'\n",
    "        )\n",
    "        \n",
    "        output = mx.sym.SoftmaxOutput(\n",
    "            data = pred,\n",
    "            name = 'softmax'\n",
    "        )\n",
    "        \n",
    "        executor = output.bind(ctx=self.ctx, args={\n",
    "            'data': x,\n",
    "            'cls_weight': self.cls_weight,\n",
    "            'cls_bias'  : self.cls_bias,\n",
    "            'softmax_label': mx.nd.array([0]) # this is a dummy label, just meant to fulfill the requirements...\n",
    "        })\n",
    "        \n",
    "        executor.forward()\n",
    "        prob = np.squeeze(executor.outputs[0].asnumpy())\n",
    "        return prob\n",
    "        \n",
    "    def forward(self, input_data, new_seq=False):\n",
    "        # input data is of shape (seqlen, dim)\n",
    "        # input data has to be of type numpy.array\n",
    "        if new_seq == True:\n",
    "            # this is meant to reset the initial states to 0.0\n",
    "            for key in self.states_dict.keys():\n",
    "                self.executor.arg_dict[key][:] = 0.0\n",
    "        \n",
    "        for x in input_data:\n",
    "            y = mx.nd.array([x]) # put it in a [] so that the shape becomes (1, xxx)\n",
    "            y.copyto(self.executor.arg_dict[\"data\"])\n",
    "            self.executor.forward() # move forward one step...\n",
    "            for key in self.states_dict.keys():\n",
    "                # copy the hidden and c to the init_states for the next sequence\n",
    "#                 print('forwarding', key)\n",
    "                self.states_dict[key].copyto(self.executor.arg_dict[key])\n",
    "        \n",
    "        return self.predict(self.states_dict['l2_init_h']) # change this to use last layer next time...\n",
    "    \n",
    "    def translate(self, text, reverse=True):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = LSTMInferenceModel(num_layers, num_hidden, num_labels, arg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the word...\n",
    "def get_word(prob, idx2word, sample=True):\n",
    "    if sample:\n",
    "        cdf = np.cumsum(prob) / np.sum(prob)\n",
    "        idx = np.argmax(np.random.rand(1) < cdf)\n",
    "    else:\n",
    "        idx = np.argmax(prob)\n",
    "    return idx, idx2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate(model2, text, idx2word, reverse=True):\n",
    "    data = text_2_indices(word2idx, text)\n",
    "    if reverse:\n",
    "        data[:-1] = np.flipud(data[:-1])\n",
    "    eos_idx = word2idx['<EOS>']\n",
    "    \n",
    "    words = ''\n",
    "    prob = model2.forward(data, new_seq=True)\n",
    "    idx, word = get_word(prob, idx2word, sample=True)\n",
    "    while idx != eos_idx:\n",
    "        words += word + ' '\n",
    "        prob = model2.forward(np.array([idx]))\n",
    "        idx, word = get_word(prob, idx2word, sample=True)\n",
    "    \n",
    "    return words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'спасибо'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model2, 'good dinner', idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ужин хороший'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model2, 'i just ate', idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "я только что съел мой ужин"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
