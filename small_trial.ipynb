{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def perplexity(label, pred, ignore_label):\n",
    "    label = label.T.reshape((-1,))\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        if label[i] == ignore_label:\n",
    "            break\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_train_input='data/1.en'\n",
    "dec_train_input='data/1.ru'\n",
    "num_buckets=1\n",
    "num_layers=3\n",
    "num_hidden=10\n",
    "batch_size=1\n",
    "iterations=1\n",
    "expt_name='simple'\n",
    "params_dir='params'\n",
    "shuffle=False\n",
    "reverse=True\n",
    "top_words=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word2idx, idx2word = get_unified_vocab(enc_train_input, dec_train_input, top_words)\n",
    "train_data_label = get_data_label(enc_train_input, dec_train_input, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = EncoderDecoderIter(train_data_label, word2idx, idx2word,\n",
    "            num_hidden, num_layers, get_lstm_init_states, batch_size=batch_size,\n",
    "            num_buckets=num_buckets, shuffle=shuffle, rev=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_iter(iter):\n",
    "    iter.reset()\n",
    "    print('provide_data: ', iter.provide_data)\n",
    "    print('provide_label: ', iter.provide_label)\n",
    "    print('buckets: ', iter.buckets)\n",
    "    print('buckets count: ', iter.buckets_count)\n",
    "    print('assignments: ', iter.assignments)\n",
    "    print('batch_size: ', iter.batch_size)\n",
    "    for i, data_batch in enumerate(iter):\n",
    "        print(i, data_batch.provide_data)\n",
    "        print(i, data_batch.provide_label)\n",
    "        print(i, data_batch.bucket_key)\n",
    "        print(i, data_batch.data)\n",
    "        for j, d in enumerate(data_batch.data):\n",
    "            print(i, j, data_batch.data[j].shape)\n",
    "            if j==0:\n",
    "                print(data_batch.data[j].asnumpy())\n",
    "        print(i, data_batch.label)\n",
    "        print(i, data_batch.label[0].shape)\n",
    "        print(i, data_batch.label[0].asnumpy())\n",
    "#         print('\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context = mx.cpu()\n",
    "\n",
    "model_args = {}\n",
    "if os.path.isfile('%s/%s-symbol.json' % (params_dir, expt_name)):\n",
    "    filelist = os.listdir(params_dir) # get list of params file\n",
    "    paramfilelist = []\n",
    "    for f in filelist:\n",
    "        if f.startswith('%s-' % expt_name) and f.endswith('.params'):\n",
    "            paramfilelist.append( int(re.split(r'[-.]', f)[1]) )\n",
    "    last_iteration = max(paramfilelist)\n",
    "    print('loading pretrained model %s/%s at epoch %d' % (params_dir, expt_name, last_iteration))\n",
    "    tmp = mx.model.FeedForward.load('%s/%s' % (params_dir, expt_name), last_iteration)\n",
    "    model_args.update({\n",
    "        'arg_params' : tmp.arg_params,\n",
    "        'aux_params' : tmp.aux_params,\n",
    "        'begin_epoch' : tmp.begin_epoch\n",
    "    })\n",
    "\n",
    "num_labels = len(word2idx) - 1\n",
    "model = mx.model.FeedForward(\n",
    "    ctx           = context, # uses all the available CPU in the machine\n",
    "    symbol        = get_lstm_sym_generator(num_layers, num_hidden, num_labels),\n",
    "    num_epoch     = iterations,\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.0,\n",
    "    wd            = 0.00001,\n",
    "    initializer   = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n",
    "    **model_args\n",
    ")\n",
    "\n",
    "if not os.path.exists(params_dir):\n",
    "    os.makedirs(params_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric = mx.metric.np(perplexity, use_ignore=True, ignore_label=num_labels),\n",
    "    batch_end_callback = [ mx.callback.Speedometer(batch_size, frequent=10) ],\n",
    "    epoch_end_callback = [ mx.callback.do_checkpoint( '%s/%s' % (params_dir, expt_name) ) ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
